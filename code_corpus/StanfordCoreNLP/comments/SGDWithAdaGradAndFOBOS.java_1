/**
 * Stochastic Gradient Descent With AdaGrad and FOBOS in batch mode.
 * Optionally, user can also turn on AdaDelta via option "useAdaDelta"
 * Similar to SGDMinimizer, regularization is done in the minimizer, not in the objective function.
 * This version is not efficient for online setting. For online variant, consider implementing SparseAdaGradMinimizer.java
 *
 * @author Mengqiu Wang
 */
